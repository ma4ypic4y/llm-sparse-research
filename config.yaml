# Configuration for sparse weights training
mode: 'masked-weights' # 'none' | 'masked-activations' | 'masked-weights' | 'masked-activations-layer'

collector:
  zero_weight_threshold: 0.0
  dead_grad_threshold: 0.0
  s_collect_frequency: 10
  dump_frequency: 100
  collect_weights: false

# Pruning parameters
pruning:
  target_sparsity: 0.2
  warmup_ratio: 0.15
  final_prune_ratio: 0.85
  prune_applications: 12

# Training parameters (optimized for GPU memory constraints)
training:
  callbacks: ['s-collector', 'm-collector'] # 's-collector', 'm-collector' | weights statistics collector; masks statistics collector
  report_to: ['wandb'] # 'wandb'
  epochs: 3
  batch_size: 64
  seq_len: 256
  lr: 0.0001
  device: 'cuda'
  use_bf16: true
  eval_steps: 10
  infer_text: 'To be, or not to be'
  dataset: 'shakespeare' # Only 'shakespeare' | 'wikitext' | 'red_pajama'

paths:
  output_dir: './exp'
  summary_dir: './summary'

# Model settings
model:
  config_name: 'nanoGPT' # 'gpt2' | 'nanoGPT' | 'neoGPT'
  tokenizer_name: 'gpt2'
