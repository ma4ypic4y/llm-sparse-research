# Configuration for sparse weights training
mode: 'masked-activations-layer' # One of: 'none', 'masked-activations', 'masked-activations-layer', 'masked-weights', 'sparse-activations', 'sparse-weights'

collector:
  zero_weight_threshold: 0.0
  dead_grad_threshold: 0.0
  s_collect_frequency: 8
  dump_frequency: 250
  collect_weights: false

# Pruning parameters
pruning:
  target_sparsity: 0.2
  warmup_ratio: 0.15
  final_prune_ratio: 0.85
  prune_applications: 12

# Training parameters (optimized for GPU memory constraints)
training:
  callbacks: ['s-collector'] # 's-collector', 'm-collector' | weights statistics collector; masks statistics collector
  report_to: ['wandb']
  epochs: 20
  batch_size: 64
  seq_len: 256
  lr: 0.0001
  device: 'cuda'
  use_bf16: false
  eval_steps: 10
  infer_text: 'To be, or not to be'

paths:
  output_dir: './exp'
  summary_dir: './summary'

# Model settings
model:
  config_name: 'gpt-neo-125M' # gpt-neo-125M | gpt2
  tokenizer_name: 'gpt2'
  hidden_size: 384
  num_layers: 6
  num_heads: 6
  intermediate_size: 1536
