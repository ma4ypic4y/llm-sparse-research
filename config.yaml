# Configuration for sparse weights training
mode: 'masked-weights' # One of: 'none', 'masked-activations', 'masked-weights', 'sparse-activations', 'sparse-weights'

wandb:
  enabled: true
  project: 'llm-sparse-research-none'

collector:
  zero_weight_threshold: 0.0
  dead_grad_threshold: 0.0
  trackable_weights_layers: None
  trackable_masks: None
  collect_frequency: 1
  dump_frequency: 100
  collect_weights: false

# Pruning parameters
pruning:
  target_sparsity: 0.1
  warmup_ratio: 0.15
  final_prune_ratio: 0.85
  prune_applications: 12

# Training parameters (optimized for GPU memory constraints)
training:
  callbacks: [] # 'data-collector'
  report_to: ['wandb']
  epochs: 20
  batch_size: 8
  seq_len: 128
  lr: 0.00005
  device: 'cpu'
  use_bf16: true

paths:
  output_dir: './exp'

# Model settings
model:
  config_name: 'gpt2'
  tokenizer_name: 'gpt2'
