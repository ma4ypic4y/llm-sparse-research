# Configuration for sparse weights training
mode: 'none' # 'none' | 'masked-activations' | 'masked-weights' | 'masked-activations-layer'

collector:
  zero_weight_threshold: 0.0
  dead_grad_threshold: 0.0
  s_collect_frequency: 10
  dump_frequency: 100
  collect_weights: false

# Pruning parameters
pruning:
  target_sparsity: 0.2
  warmup_ratio: 0.15
  final_prune_ratio: 0.85
  prune_applications: 12

# Training parameters (optimized for GPU memory constraints)
training:
  callbacks: ['s-collector', 'm-collector'] # 's-collector', 'm-collector' | weights statistics collector; masks statistics collector
  report_to: ['wandb'] # 'wandb'
  epochs: 1
  batch_size: 64
  gradient_accumulation_steps: 1
  seq_len: 256
  lr: 0.0005
  device: 'cuda'
  use_bf16: true
  eval_steps: 250
  infer_text: 'To be or not to be'
  dataset: 'shakespeare' # 'shakespeare' | 'wikitext' | 'red_pajama'

paths:
  output_dir: './exp'
  summary_dir: './summary'

# Model settings
model:
  model_name: 'gpt-nano' # 'gpt-nano' | 'gpt-neo' | 'gpt-small' | 'llama3'
  tokenizer_name: 'char' # 'gpt' | 'llama' | 'char'
